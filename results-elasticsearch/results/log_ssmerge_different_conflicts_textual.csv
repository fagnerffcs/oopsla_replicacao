file;linedbasedConf
/home/paper219/Desktop/analysis/fullsample/projects/elasticsearch-river-mongodb/revisions/rev_2f13341_eae373b/rev_2f13341-eae373b/src/main/java/org/elasticsearch/river/mongodb/MongoDBRiver.java;<<<<<<< MINE
	public final static String LOCAL_DB_FIELD = "local";
	public final static String ADMIN_DB_FIELD = "admin";
=======
	public final static String DB_LOCAL = "local";
	public final static String DB_ADMIN = "admin";
	public final static String DB_CONFIG = "config";
>>>>>>> YOURS
/home/paper219/Desktop/analysis/fullsample/projects/elasticsearch-river-mongodb/revisions/rev_3cd95d1_6f57fc1/rev_3cd95d1-6f57fc1/src/main/java/org/elasticsearch/river/mongodb/MongoDBRiver.java;<<<<<<< MINE
	protected final Set<String> excludeFields;
	protected final String includeCollection;
=======
	protected final BasicDBObject excludeFields;
>>>>>>> YOURS
/home/paper219/Desktop/analysis/fullsample/projects/elasticsearch-river-mongodb/revisions/rev_3ca5723_c520223/rev_3ca5723-c520223/src/main/java/org/elasticsearch/river/mongodb/MongoDBRiver.java;<<<<<<< MINE
=======
import com.mongodb.gridfs.GridFS;
import com.mongodb.gridfs.GridFSDBFile;
import com.mongodb.gridfs.GridFSFile;
>>>>>>> YOURS
/home/paper219/Desktop/analysis/fullsample/projects/elasticsearch-river-mongodb/revisions/rev_3ca5723_c520223/rev_3ca5723-c520223/src/main/java/org/elasticsearch/river/mongodb/MongoDBRiver.java;<<<<<<< MINE
			this.context.setActive(false);
=======
			active = false;
		}
	}

	private class Indexer implements Runnable {

		private final ESLogger logger = ESLoggerFactory.getLogger(this
				.getClass().getName());
		private int deletedDocuments = 0;
		private int insertedDocuments = 0;
		private int updatedDocuments = 0;
		private StopWatch sw;
		private ExecutableScript scriptExecutable;

		@Override
		public void run() {
			while (active) {
				sw = new StopWatch().start();
				deletedDocuments = 0;
				insertedDocuments = 0;
				updatedDocuments = 0;

				if (definition.getScript() != null
						&& definition.getScriptType() != null) {
					scriptExecutable = scriptService.executable(
							definition.getScriptType(), definition.getScript(),
							ImmutableMap.of("logger", logger));
				}

				try {
					BSONTimestamp lastTimestamp = null;
					BulkRequestBuilder bulk = client.prepareBulk();

					// 1. Attempt to fill as much of the bulk request as possible
					QueueEntry entry = stream.take();
					lastTimestamp = processBlockingQueue(bulk, entry);
					while ((entry = stream.poll(definition.getBulkTimeout()
							.millis(), MILLISECONDS)) != null) {
						lastTimestamp = processBlockingQueue(bulk, entry);
						if (bulk.numberOfActions() >= definition.getBulkSize()) {
							break;
						}
					}

					// 2. Update the timestamp
					if (lastTimestamp != null) {
						updateLastTimestamp(mongoOplogNamespace, lastTimestamp, bulk);
					}

					// 3. Execute the bulk requests
					try {
						BulkResponse response = bulk.execute().actionGet();
						if (response.hasFailures()) {
							// TODO write to exception queue?
							logger.warn("failed to execute"
									+ response.buildFailureMessage());
						}
					} catch (ElasticSearchInterruptedException esie) {
						logger.warn(
								"river-mongodb indexer bas been interrupted",
								esie);
						Thread.currentThread().interrupt();
					} catch (Exception e) {
						logger.warn("failed to execute bulk", e);
					}

				} catch (InterruptedException e) {
					if (logger.isDebugEnabled()) {
						logger.debug("river-mongodb indexer interrupted");
					}
					Thread.currentThread().interrupt();
					break;
				}
				logStatistics();
			}
		}

		@SuppressWarnings({ "unchecked" })
		private BSONTimestamp processBlockingQueue(
				final BulkRequestBuilder bulk, QueueEntry entry) {
			if (entry.getData().get(MONGODB_ID_FIELD) == null
					&& !entry.getOperation().equals(
							OPLOG_COMMAND_OPERATION)) {
				logger.warn(
						"Cannot get object id. Skip the current item: [{}]",
						entry.getData());
				return null;
			}

			BSONTimestamp lastTimestamp = entry.getOplogTimestamp();
			String operation = entry.getOperation();
			if (OPLOG_COMMAND_OPERATION.equals(operation)) {
				try {
					updateBulkRequest(bulk, entry.getData(), null, operation, 
							definition.getIndexName(),
							definition.getTypeName(), null, null);
				} catch (IOException ioEx) {
					logger.error("Update bulk failed.", ioEx);
				}
				return lastTimestamp;
			}

			String objectId = "";
			if (entry.getData().get(MONGODB_ID_FIELD) != null) {
				objectId = entry.getData().get(MONGODB_ID_FIELD).toString();
			}

			// TODO: Should the river support script filter,
			// advanced_transformation, include_collection for GridFS?
			if (entry.isAttachment()) {
				try {
					updateBulkRequest(bulk, entry.getData(), objectId,
							operation, definition.getIndexName(),
							definition.getTypeName(), null, null);
				} catch (IOException ioEx) {
					logger.error("Update bulk failed.", ioEx);
				}
				return lastTimestamp;
			}
			
			if (scriptExecutable != null
					&& definition.isAdvancedTransformation()) {
				return applyAdvancedTransformation(bulk, entry);
			}

			if (logger.isDebugEnabled()) {
				logger.debug("updateBulkRequest for id: [{}], operation: [{}]",
						objectId, operation);
			}

			if (!definition.getIncludeCollection().isEmpty()) {
				logger.trace(
						"About to include collection. set attribute {} / {} ",
						definition.getIncludeCollection(),
						definition.getMongoCollection());
				entry.getData().put(definition.getIncludeCollection(),
						definition.getMongoCollection());
			}

			Map<String, Object> ctx = null;
			try {
				ctx = XContentFactory.xContent(XContentType.JSON)
						.createParser("{}").mapAndClose();
			} catch (IOException e) {
				logger.warn("failed to parse {}", e);
			}
			Map<String, Object> data = entry.getData().toMap();
			if (scriptExecutable != null) {
				if (ctx != null) {
					ctx.put("document", entry.getData());
					ctx.put("operation", operation);
					if (!objectId.isEmpty()) {
						ctx.put("id", objectId);
					}
					if (logger.isDebugEnabled()) {
						logger.debug("Script to be executed: {}",
								scriptExecutable);
						logger.debug("Context before script executed: {}", ctx);
					}
					scriptExecutable.setNextVar("ctx", ctx);
					try {
						scriptExecutable.run();
						// we need to unwrap the context object...
						ctx = (Map<String, Object>) scriptExecutable
								.unwrap(ctx);
					} catch (Exception e) {
						logger.warn("failed to script process {}, ignoring", e,
								ctx);
					}
					if (logger.isDebugEnabled()) {
						logger.debug("Context after script executed: {}", ctx);
					}
					if (ctx.containsKey("ignore")
							&& ctx.get("ignore").equals(Boolean.TRUE)) {
						logger.debug("From script ignore document id: {}",
								objectId);
						// ignore document
						return lastTimestamp;
					}
					if (ctx.containsKey("deleted")
							&& ctx.get("deleted").equals(Boolean.TRUE)) {
						ctx.put("operation", OPLOG_DELETE_OPERATION);
					}
					if (ctx.containsKey("document")) {
						data = (Map<String, Object>) ctx.get("document");
						logger.debug("From script document: {}", data);
					}
					if (ctx.containsKey("operation")) {
						operation = ctx.get("operation").toString();
						logger.debug("From script operation: {}", operation);
					}
				}
			}

			try {
				String index = extractIndex(ctx);
				String type = extractType(ctx);
				String parent = extractParent(ctx);
				String routing = extractRouting(ctx);
				objectId = extractObjectId(ctx, objectId);
				updateBulkRequest(bulk, new BasicDBObject(data), objectId, operation, index, type,
						routing, parent);
			} catch (IOException e) {
				logger.warn("failed to parse {}", e, entry.getData());
			}
			return lastTimestamp;
		}

		private void updateBulkRequest(final BulkRequestBuilder bulk,
				DBObject data, String objectId, String operation, String index,
				String type, String routing, String parent) throws IOException {
			if (logger.isDebugEnabled()) {
				logger.debug(
						"Operation: {} - index: {} - type: {} - routing: {} - parent: {}",
						operation, index, type, routing, parent);
			}
			boolean isAttachment = false;
			
			if (logger.isDebugEnabled()) {
				isAttachment = (data instanceof GridFSDBFile);
			}
			if (OPLOG_INSERT_OPERATION.equals(operation)) {
				if (logger.isDebugEnabled()) {
					logger.debug(
							"Insert operation - id: {} - contains attachment: {}",
							operation, objectId, isAttachment);
				}
				bulk.add(indexRequest(index).type(type).id(objectId)
						.source(build(data, objectId)).routing(routing)
						.parent(parent));
				insertedDocuments++;
			}
			if (OPLOG_UPDATE_OPERATION.equals(operation)) {
				if (logger.isDebugEnabled()) {
					logger.debug(
							"Update operation - id: {} - contains attachment: {}",
							objectId, isAttachment);
				}
				deleteBulkRequest(bulk, objectId, index, type, routing, parent);
				bulk.add(indexRequest(index).type(type).id(objectId)
						.source(build(data, objectId)).routing(routing)
						.parent(parent));
				updatedDocuments++;
			}
			if (OPLOG_DELETE_OPERATION.equals(operation)) {
				logger.info("Delete request [{}], [{}], [{}]", index, type,
						objectId);
				deleteBulkRequest(bulk, objectId, index, type, routing, parent);
				deletedDocuments++;
			}
			if (OPLOG_COMMAND_OPERATION.equals(operation)) {
				if (definition.isDropCollection()) {
					if (data.get(OPLOG_DROP_COMMAND_OPERATION) != null
							&& data.get(OPLOG_DROP_COMMAND_OPERATION).equals(
									definition.getMongoCollection())) {
						logger.info("Drop collection request [{}], [{}]",
								index, type);
						bulk.request().requests().clear();
						client.admin().indices().prepareRefresh(index)
								.execute().actionGet();
						Map<String, MappingMetaData> mappings = client.admin()
								.cluster().prepareState().execute().actionGet()
								.getState().getMetaData().index(index)
								.mappings();
						logger.trace("mappings contains type {}: {}", type,
								mappings.containsKey(type));
						if (mappings.containsKey(type)) {
							/*
							 * Issue #105 - Mapping changing from custom mapping
							 * to dynamic when drop_collection = true Should
							 * capture the existing mapping metadata (in case it
							 * is has been customized before to delete.
							 */
							MappingMetaData mapping = mappings.get(type);
							client.admin().indices()
									.prepareDeleteMapping(index).setType(type)
									.execute().actionGet();
							PutMappingResponse pmr = client.admin().indices()
									.preparePutMapping(index).setType(type)
									.setSource(mapping.source().string())
									.execute().actionGet();
							if (!pmr.isAcknowledged()) {
								logger.error(
										"Failed to put mapping {} / {} / {}.",
										index, type, mapping.source());
							}
						}

						deletedDocuments = 0;
						updatedDocuments = 0;
						insertedDocuments = 0;
						logger.info(
								"Delete request for index / type [{}] [{}] successfully executed.",
								index, type);
					} else {
						logger.debug("Database command {}", data);
					}
				} else {
					logger.info(
							"Ignore drop collection request [{}], [{}]. The option has been disabled.",
							index, type);
				}
			}

		}

		/*
		 * Delete children when parent / child is used
		 */
		private void deleteBulkRequest(BulkRequestBuilder bulk,
				String objectId, String index, String type, String routing,
				String parent) {
			logger.trace(
					"bulkDeleteRequest - objectId: {} - index: {} - type: {} - routing: {} - parent: {}",
					objectId, index, type, routing, parent);

			if (definition.getParentTypes() != null
					&& definition.getParentTypes().contains(type)) {
				QueryBuilder builder = QueryBuilders.hasParentQuery(type,
						QueryBuilders.termQuery(MONGODB_ID_FIELD, objectId));
				SearchResponse response = client.prepareSearch(index)
						.setQuery(builder).setRouting(routing)
						.addField(MONGODB_ID_FIELD).execute().actionGet();
				for (SearchHit hit : response.getHits().getHits()) {
					bulk.add(deleteRequest(index).type(hit.getType())
							.id(hit.getId()).routing(routing).parent(objectId));
				}
			}
			bulk.add(deleteRequest(index).type(type).id(objectId)
					.routing(routing).parent(parent));
		}

		@SuppressWarnings("unchecked")
		private BSONTimestamp applyAdvancedTransformation(
				final BulkRequestBuilder bulk, QueueEntry entry) {

			BSONTimestamp lastTimestamp = entry.getOplogTimestamp();
			String operation = entry.getOperation();
			String objectId = "";
			if (entry.getData().get(MONGODB_ID_FIELD) != null) {
				objectId = entry.getData().get(MONGODB_ID_FIELD).toString();
			}
			if (logger.isDebugEnabled()) {
				logger.debug(
						"advancedUpdateBulkRequest for id: [{}], operation: [{}]",
						objectId, operation);
			}

			if (!definition.getIncludeCollection().isEmpty()) {
				logger.trace(
						"About to include collection. set attribute {} / {} ",
						definition.getIncludeCollection(),
						definition.getMongoCollection());
				entry.getData().put(definition.getIncludeCollection(),
						definition.getMongoCollection());
			}
			Map<String, Object> ctx = null;
			try {
				ctx = XContentFactory.xContent(XContentType.JSON)
						.createParser("{}").mapAndClose();
			} catch (Exception e) {
			}

			List<Object> documents = new ArrayList<Object>();
			Map<String, Object> document = new HashMap<String, Object>();

			if (scriptExecutable != null) {
				if (ctx != null && documents != null) {

					document.put("data", entry.getData().toMap());
					if (!objectId.isEmpty()) {
						document.put("id", objectId);
					}
					document.put("_index", definition.getIndexName());
					document.put("_type", definition.getTypeName());
					document.put("operation", operation);

					documents.add(document);

					ctx.put("documents", documents);
					if (logger.isDebugEnabled()) {
						logger.debug("Script to be executed: {}",
								scriptExecutable);
						logger.debug("Context before script executed: {}", ctx);
					}
					scriptExecutable.setNextVar("ctx", ctx);
					try {
						scriptExecutable.run();
						// we need to unwrap the context object...
						ctx = (Map<String, Object>) scriptExecutable
								.unwrap(ctx);
					} catch (Exception e) {
						logger.warn("failed to script process {}, ignoring", e,
								ctx);
					}
					if (logger.isDebugEnabled()) {
						logger.debug("Context after script executed: {}", ctx);
					}
					if (ctx.containsKey("documents")
							&& ctx.get("documents") instanceof List<?>) {
						documents = (List<Object>) ctx.get("documents");
						for (Object object : documents) {
							if (object instanceof Map<?, ?>) {
								Map<String, Object> item = (Map<String, Object>) object;
								logger.trace("item: {}", item);
								if (item.containsKey("deleted")
										&& item.get("deleted").equals(
												Boolean.TRUE)) {
									item.put("operation",
											OPLOG_DELETE_OPERATION);
								}

								String index = extractIndex(item);
								String type = extractType(item);
								String parent = extractParent(item);
								String routing = extractRouting(item);
								String action = extractOperation(item);
								boolean ignore = isDocumentIgnored(item);
								Map<String, Object> _data = (Map<String, Object>) item
										.get("data");
								objectId = extractObjectId(_data, objectId);
								if (logger.isDebugEnabled()) {
									logger.debug(
											"Id: {} - operation: {} - ignore: {} - index: {} - type: {} - routing: {} - parent: {}",
											objectId, action, ignore, index,
											type, routing, parent);
								}
								if (ignore) {
									continue;
								}
								try {
									updateBulkRequest(bulk, new BasicDBObject(_data), objectId,
											operation, index, type, routing,
											parent);
								} catch (IOException ioEx) {
									logger.error("Update bulk failed.", ioEx);
								}
							}
						}
					}
				}
			}

			return lastTimestamp;
		}

		private XContentBuilder build(final DBObject data, final String objectId)
				throws IOException {
			if (data instanceof GridFSDBFile) {
				logger.info("Add Attachment: {} to index {} / type {}",
						objectId, definition.getIndexName(),
						definition.getTypeName());
				return MongoDBHelper.serialize((GridFSDBFile) data);
			} else {
				return XContentFactory.jsonBuilder().map(data.toMap());
			}
		}

		private String extractObjectId(Map<String, Object> ctx, String objectId) {
			Object id = ctx.get("id");
			if (id != null) {
				return id.toString();
			}
			id = ctx.get(MONGODB_ID_FIELD);
			if (id != null) {
				return id.toString();
			} else {
				return objectId;
			}
		}

		private String extractParent(Map<String, Object> ctx) {
			Object parent = ctx.get("_parent");
			if (parent == null) {
				return null;
			} else {
				return parent.toString();
			}
		}

		private String extractRouting(Map<String, Object> ctx) {
			Object routing = ctx.get("_routing");
			if (routing == null) {
				return null;
			} else {
				return routing.toString();
			}
		}

		private String extractOperation(Map<String, Object> ctx) {
			Object operation = ctx.get("operation");
			if (operation == null) {
				return null;
			} else {
				return operation.toString();
			}
		}

		private boolean isDocumentIgnored(Map<String, Object> ctx) {
			return (ctx.containsKey("ignore") && ctx.get("ignore").equals(
					Boolean.TRUE));
		}

		private String extractType(Map<String, Object> ctx) {
			Object type = ctx.get("_type");
			if (type == null) {
				return definition.getTypeName();
			} else {
				return type.toString();
			}
		}

		private String extractIndex(Map<String, Object> ctx) {
			String index = (String) ctx.get("_index");
			if (index == null) {
				index = definition.getIndexName();
			}
			return index;
		}

		private void logStatistics() {
			long totalDocuments = deletedDocuments + insertedDocuments;
			long totalTimeInSeconds = sw.stop().totalTime().seconds();
			long totalDocumentsPerSecond = (totalTimeInSeconds == 0) ? totalDocuments
					: totalDocuments / totalTimeInSeconds;
			logger.info(
					"Indexed {} documents, {} insertions, {} updates, {} deletions, {} documents per second",
					totalDocuments, insertedDocuments, updatedDocuments,
					deletedDocuments, totalDocumentsPerSecond);
		}
	}

	private class Slurper implements Runnable {

		private Mongo mongo;
		private DB slurpedDb;
		private DBCollection slurpedCollection;
		private DB oplogDb;
		private DBCollection oplogCollection;

		public Slurper(List<ServerAddress> mongoServers) {
			this.mongo = new MongoClient(mongoServers,
				definition.getMongoClientOptions());
		}

		@Override
		public void run() {
			while (active) {
				try {
					if (!assignCollections()) {
						break;	// failed to assign oplogCollection or
								// slurpedCollection
					}

					DBCursor cursor = null;

					BSONTimestamp startTimestamp = null;

					// Do an initial sync the same way MongoDB does
					// https://groups.google.com/forum/?fromgroups=#!topic/mongodb-user/sOKlhD_E2ns
					// TODO: support GridFS
					BSONTimestamp lastIndexedTimestamp = getLastTimestamp(mongoOplogNamespace);
					if (lastIndexedTimestamp == null) {
						// TODO: ensure the index type is empty
						logger.info("MongoDBRiver is beginning initial import of "
							+ slurpedCollection.getFullName());
						startTimestamp = getCurrentOplogTimestamp();
						try {
							if (!definition.isMongoGridFS()) {
								cursor = slurpedCollection.find();
								while (cursor.hasNext()) {
									DBObject object = cursor.next();
									addToStream(OPLOG_INSERT_OPERATION, null, applyFieldFilter(object));
								}	
							} else {
								// TODO: To be optimized. https://github.com/mongodb/mongo-java-driver/pull/48#issuecomment-25241988
								// possible option: Get the object id list from .fs collection then call GriDFS.findOne
								GridFS grid = new GridFS(mongo.getDB(definition.getMongoDb()),
										definition.getMongoCollection());
								cursor = grid.getFileList();
								while (cursor.hasNext()) {
									DBObject object = cursor.next();
									if (object instanceof GridFSDBFile) {
										GridFSDBFile file = grid.findOne(new ObjectId(object.get(MONGODB_ID_FIELD).toString()));
										addToStream(OPLOG_INSERT_OPERATION, null, file);
									}
								}
							}
						} finally {
							if (cursor != null) {
								logger.trace("Closing initial import cursor");
								cursor.close();
							}
						}
					}

					// Slurp from oplog
					try {
						cursor = oplogCursor(startTimestamp);
						if (cursor == null) {
							cursor = processFullOplog();
						}
						while (cursor.hasNext()) {
							DBObject item = cursor.next();
							processOplogEntry(item);
						}
						Thread.sleep(500);
					} finally {
						if (cursor != null) {
							logger.trace("Closing oplog cursor");
							cursor.close();
						}
					}
				} catch (MongoInterruptedException mIEx) {
					logger.warn("Mongo driver has been interrupted");
					if (mongo != null) {
						mongo.close();
						mongo = null;
					}
					break;
				} catch (MongoException mEx) {
					logger.error("Mongo gave an exception", mEx);
				} catch (NoSuchElementException nEx) {
					logger.warn("A mongoDB cursor bug ?", nEx);
				} catch (InterruptedException e) {
					if (logger.isDebugEnabled()) {
						logger.debug("river-mongodb slurper interrupted");
					}
					Thread.currentThread().interrupt();
					break;
				}
			}
		}

		private boolean assignCollections() {
			DB adminDb = mongo.getDB(MONGODB_ADMIN_DATABASE);
			oplogDb = mongo.getDB(MONGODB_LOCAL_DATABASE);

			if (!definition.getMongoAdminUser().isEmpty()
					&& !definition.getMongoAdminPassword().isEmpty()) {
				logger.info("Authenticate {} with {}", MONGODB_ADMIN_DATABASE,
						definition.getMongoAdminUser());

				CommandResult cmd = adminDb.authenticateCommand(definition
						.getMongoAdminUser(), definition
						.getMongoAdminPassword().toCharArray());
				if (!cmd.ok()) {
					logger.error("Autenticatication failed for {}: {}",
							MONGODB_ADMIN_DATABASE, cmd.getErrorMessage());
					// Can still try with mongoLocal credential if provided.
					// return false;
				}
				oplogDb = adminDb.getMongo().getDB(MONGODB_LOCAL_DATABASE);
			}

			if (!definition.getMongoLocalUser().isEmpty()
					&& !definition.getMongoLocalPassword().isEmpty()
					&& !oplogDb.isAuthenticated()) {
				logger.info("Authenticate {} with {}", MONGODB_LOCAL_DATABASE,
						definition.getMongoLocalUser());
				CommandResult cmd = oplogDb.authenticateCommand(definition
						.getMongoLocalUser(), definition
						.getMongoLocalPassword().toCharArray());
				if (!cmd.ok()) {
					logger.error("Autenticatication failed for {}: {}",
							MONGODB_LOCAL_DATABASE, cmd.getErrorMessage());
					return false;
				}
			}

			Set<String> collections = oplogDb.getCollectionNames();
			if (!collections.contains(OPLOG_COLLECTION)) {
				logger.error("Cannot find "
						+ OPLOG_COLLECTION
						+ " collection. Please check this link: http://goo.gl/2x5IW");
				return false;
			}
			oplogCollection = oplogDb.getCollection(OPLOG_COLLECTION);

			slurpedDb = mongo.getDB(definition.getMongoDb());
			if (!definition.getMongoAdminUser().isEmpty()
					&& !definition.getMongoAdminPassword().isEmpty()
					&& adminDb.isAuthenticated()) {
				slurpedDb = adminDb.getMongo().getDB(definition.getMongoDb());
			}

			// Not necessary as local user has access to all databases.
			// http://docs.mongodb.org/manual/reference/local-database/
			// if (!mongoDbUser.isEmpty() && !mongoDbPassword.isEmpty()
			// && !slurpedDb.isAuthenticated()) {
			// logger.info("Authenticate {} with {}", mongoDb, mongoDbUser);
			// CommandResult cmd = slurpedDb.authenticateCommand(mongoDbUser,
			// mongoDbPassword.toCharArray());
			// if (!cmd.ok()) {
			// logger.error("Authentication failed for {}: {}",
			// mongoDb, cmd.getErrorMessage());
			// return false;
			// }
			// }
			slurpedCollection = slurpedDb.getCollection(definition
					.getMongoCollection());

			return true;
		}

		private BSONTimestamp getCurrentOplogTimestamp() {
			return (BSONTimestamp) oplogCollection
					.find()
					.sort(new BasicDBObject(OPLOG_TIMESTAMP, -1))
					.limit(1)
					.next()
					.get(OPLOG_TIMESTAMP);
		}

		private DBCursor processFullOplog() throws InterruptedException {
			BSONTimestamp currentTimestamp = getCurrentOplogTimestamp();
			addQueryToStream(OPLOG_INSERT_OPERATION, currentTimestamp, null);
			return oplogCursor(currentTimestamp);
		}

		private void processOplogEntry(final DBObject entry)
				throws InterruptedException {
			String operation = entry.get(OPLOG_OPERATION).toString();
			String namespace = entry.get(OPLOG_NAMESPACE).toString();
			BSONTimestamp oplogTimestamp = (BSONTimestamp) entry
					.get(OPLOG_TIMESTAMP);
			DBObject object = (DBObject) entry.get(OPLOG_OBJECT);

			if (logger.isTraceEnabled()) {
				logger.trace("MongoDB object deserialized: {}",
						object.toString());
			}

			// Initial support for sharded collection -
			// https://jira.mongodb.org/browse/SERVER-4333
			// Not interested in operation from migration or sharding
			if (entry.containsField(OPLOG_FROM_MIGRATE)
					&& ((BasicBSONObject) entry).getBoolean(OPLOG_FROM_MIGRATE)) {
				logger.debug(
						"From migration or sharding operation. Can be ignored. {}",
						entry);
				return;
			}
			// Not interested by chunks - skip all
			if (namespace.endsWith(GRIDFS_CHUNKS_SUFFIX)) {
				return;
			}

			if (logger.isTraceEnabled()) {
				logger.trace("oplog entry - namespace [{}], operation [{}]",
						namespace, operation);
				logger.trace("oplog processing item {}", entry);
			}

			String objectId = getObjectIdFromOplogEntry(entry);
			if (definition.isMongoGridFS()
					&& namespace.endsWith(GRIDFS_FILES_SUFFIX)
					&& (OPLOG_INSERT_OPERATION.equals(operation) || OPLOG_UPDATE_OPERATION
							.equals(operation))) {
				if (objectId == null) {
					throw new NullPointerException(MONGODB_ID_FIELD);
				}
				GridFS grid = new GridFS(mongo.getDB(definition.getMongoDb()),
						definition.getMongoCollection());
				GridFSDBFile file = grid.findOne(new ObjectId(objectId));
				if (file != null) {
					logger.info("Caught file: {} - {}", file.getId(),
							file.getFilename());
					object = file;
				} else {
					logger.warn("Cannot find file from id: {}", objectId);
				}
			}

			if (object instanceof GridFSDBFile) {
				if (objectId == null) {
					throw new NullPointerException(MONGODB_ID_FIELD);
				}
				logger.info("Add attachment: {}", objectId);
				addToStream(operation, oplogTimestamp, applyFieldFilter(object));
			} else {
				if (OPLOG_UPDATE_OPERATION.equals(operation)) {
					DBObject update = (DBObject) entry.get(OPLOG_UPDATE);
					logger.debug("Updated item: {}", update);
					addQueryToStream(operation, oplogTimestamp, update);
				} else {
					addToStream(operation, oplogTimestamp, applyFieldFilter(object));
				}
			}
		}

		private DBObject applyFieldFilter(DBObject object) {
			if (object instanceof GridFSFile) {
				GridFSFile file = (GridFSFile) object;
				DBObject metadata = file.getMetaData();
				if (metadata != null) {
					file.setMetaData(applyFieldFilter(metadata));
				}
			} else {
				object = MongoDBHelper.applyExcludeFields(object,
						definition.getExcludeFields());
				object = MongoDBHelper.applyIncludeFields(object,
						definition.getIncludeFields());
			}
			return object;
		}

		/*
		 * Extract "_id" from "o" if it fails try to extract from "o2"
		 */
		private String getObjectIdFromOplogEntry(DBObject entry) {
			if (entry.containsField(OPLOG_OBJECT)) {
				DBObject object = (DBObject) entry.get(OPLOG_OBJECT);
				if (object.containsField(MONGODB_ID_FIELD)) {
					return object.get(MONGODB_ID_FIELD).toString();
				}
			}
			if (entry.containsField(OPLOG_UPDATE)) {
				DBObject object = (DBObject) entry.get(OPLOG_UPDATE);
				if (object.containsField(MONGODB_ID_FIELD)) {
					return object.get(MONGODB_ID_FIELD).toString();
				}
			}
			logger.trace("Oplog entry {}", entry);
			return null;
		}

		private DBObject getOplogFilter(final BSONTimestamp time) {
			BasicDBObject filter = new BasicDBObject();
			List<DBObject> values2 = new ArrayList<DBObject>();

			if (time == null) {
				logger.info("No known previous slurping time for this collection");
			} else {
				filter.put(OPLOG_TIMESTAMP, new BasicDBObject(
						QueryOperators.GT, time));
			}

			if (definition.isMongoGridFS()) {
				filter.put(OPLOG_NAMESPACE, mongoOplogNamespace
						+ GRIDFS_FILES_SUFFIX);
			} else {
				values2.add(new BasicDBObject(OPLOG_NAMESPACE,
						mongoOplogNamespace));
				values2.add(new BasicDBObject(OPLOG_NAMESPACE, definition
						.getMongoDb() + "." + OPLOG_NAMESPACE_COMMAND));
				filter.put(MONGODB_OR_OPERATOR, values2);
			}
			if (!definition.getMongoFilter().isEmpty()) {
				filter.putAll(getMongoFilter());
			}
			if (logger.isDebugEnabled()) {
				logger.debug("Using filter: {}", filter);
			}
			return filter;
		}

		private DBObject getMongoFilter() {
			List<DBObject> filters = new ArrayList<DBObject>();
			List<DBObject> filters2 = new ArrayList<DBObject>();
			List<DBObject> filters3 = new ArrayList<DBObject>();
			// include delete operation
			filters.add(new BasicDBObject(OPLOG_OPERATION,
					OPLOG_DELETE_OPERATION));

			// include update, insert in filters3
			filters3.add(new BasicDBObject(OPLOG_OPERATION,
					OPLOG_UPDATE_OPERATION));
			filters3.add(new BasicDBObject(OPLOG_OPERATION,
					OPLOG_INSERT_OPERATION));

			// include or operation statement in filter2
			filters2.add(new BasicDBObject(MONGODB_OR_OPERATOR, filters3));

			// include custom filter in filters2
			filters2.add((DBObject) JSON.parse(definition.getMongoFilter()));

			filters.add(new BasicDBObject(MONGODB_AND_OPERATOR, filters2));

			return new BasicDBObject(MONGODB_OR_OPERATOR, filters);
		}

		private DBCursor oplogCursor(final BSONTimestamp timestampOverride) {
			BSONTimestamp time = timestampOverride == null
					? getLastTimestamp(mongoOplogNamespace) : timestampOverride;
			DBObject indexFilter = getOplogFilter(time);
			if (indexFilter == null) {
				return null;
			}

			int options = Bytes.QUERYOPTION_TAILABLE
					| Bytes.QUERYOPTION_AWAITDATA | Bytes.QUERYOPTION_NOTIMEOUT;

			// Using OPLOGREPLAY to improve performance:
			// https://jira.mongodb.org/browse/JAVA-771
			if (indexFilter.containsField(OPLOG_TIMESTAMP)) {
				options = options | Bytes.QUERYOPTION_OPLOGREPLAY;
			}
			return oplogCollection.find(indexFilter)
					.sort(new BasicDBObject(MONGODB_NATURAL_OPERATOR, 1))
					.setOptions(options);
		}

		private void addQueryToStream(final String operation,
				final BSONTimestamp currentTimestamp, final DBObject update)
				throws InterruptedException {
			if (logger.isDebugEnabled()) {
				logger.debug(
						"addQueryToStream - operation [{}], currentTimestamp [{}], update [{}]",
						operation, currentTimestamp, update);
			}

			for (DBObject item : slurpedCollection.find(update, findKeys)) {
				addToStream(operation, currentTimestamp, item);
			}
		}

		private void addToStream(final String operation,
				final BSONTimestamp currentTimestamp, final DBObject data)
				throws InterruptedException {
			if (logger.isDebugEnabled()) {
				logger.debug(
						"addToStream - operation [{}], currentTimestamp [{}], data [{}]",
						operation, currentTimestamp, data);
			}

			stream.put(new QueueEntry(currentTimestamp, operation, data));
		}

	}

	private class Status implements Runnable {

		@Override
		public void run() {
			while (true) {
				try {
					if (startInvoked) {
						boolean enabled = MongoDBRiverHelper.isRiverEnabled(
								client, riverName.getName());

						if (active && !enabled) {
							logger.info("About to stop river: {}",
									riverName.getName());
							close();
						}

						if (!active && enabled) {
							logger.trace("About to start river: {}",
									riverName.getName());
							start();
						}
					}
					Thread.sleep(1000L);
				} catch (InterruptedException e) {
					logger.info("Status thread interrupted", e, (Object) null);
					Thread.currentThread().interrupt();
					break;
				}

			}
>>>>>>> YOURS
